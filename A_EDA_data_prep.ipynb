{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6372bea6",
   "metadata": {},
   "source": [
    "# My attempt at creating a \"only time-based\" dataset for predicitons.\n",
    "Perhaps we can mix and match some of the features from this to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fa5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "#Initial Data\n",
    "receivals_raw_df = pd.read_csv(\"./data/kernel/receivals.csv\")\n",
    "purchase_orders_raw_df = pd.read_csv(\"./data/kernel/purchase_orders.csv\")\n",
    "materials_raw_df = pd.read_csv(\"./data/extended/materials.csv\")\n",
    "transportation_raw_df = pd.read_csv(\"./data/extended/transportation.csv\")\n",
    "\n",
    "prediction_map_raw_df = pd.read_csv(\"./data/prediction_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d2334",
   "metadata": {},
   "source": [
    "# Part 1: Preparing Data.\n",
    "We will try different structures for our dataframe. \n",
    "\n",
    "### Option 1\n",
    "In line with our predictions being \"per-day\" basis, we will create a dataframe where the net-weight per rm_id per day is aggregated, and we will also include entries for every day (even if there are 0 receivals that day). This data structure might help our models since it is much more similar to the output we want to predict, However, it is hard to incorporate info such as purchase order info.\n",
    "\n",
    "### Option 2\n",
    "Dataframe on a \"per-receival\" basis. This is less in-line with our output, but let's us keep more information. We can also use our models to make predictions of receivals and later convert to a per-day basis as a workaround."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661b752",
   "metadata": {},
   "source": [
    "## 1.1 Data Cleaning.\n",
    "Some data-cleanup is required before going with either option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036c3367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique IDs:  203\n",
      "-57  entries with invalid rm ids dropped\n",
      "-3  duplicates dropped\n",
      "-13  entries with invalid weights dropped\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "valid_ids = prediction_map_raw_df[\"rm_id\"].unique()\n",
    "print(\"Unique IDs: \", len(valid_ids))\n",
    "\n",
    "valid_receivals = receivals_raw_df[receivals_raw_df[\"rm_id\"].isin(valid_ids)]\n",
    "print(len(valid_receivals[\"rm_id\"])- len(receivals_raw_df[\"rm_id\"]), \" entries with invalid rm ids dropped\")\n",
    "\n",
    "#Remove duplicates.\n",
    "drop_duplicates = valid_receivals.drop_duplicates()\n",
    "print(len(drop_duplicates[\"rm_id\"])- len(valid_receivals[\"rm_id\"]), \" duplicates dropped\")\n",
    "\n",
    "#Remove entries where net weight is undefined.\n",
    "valid_weights = drop_duplicates.dropna(subset=[\"net_weight\"])\n",
    "print(len(valid_weights[\"rm_id\"])- len(drop_duplicates[\"rm_id\"]), \" entries with invalid weights dropped\")\n",
    "\n",
    "#Merge with purchase orders\n",
    "receival_purchase_order_df = valid_weights.merge(purchase_orders_raw_df, on=['purchase_order_id', 'purchase_order_item_no'], how=\"left\")\n",
    "\n",
    "#Correct for different weight units\n",
    "receival_purchase_order_df.loc[receival_purchase_order_df['unit'] == 'PUND', 'net_weight'] *= 0.453592\n",
    "receival_purchase_order_df.loc[receival_purchase_order_df['unit'].isna(), 'unit'] = 'kg'\n",
    "receival_purchase_order_df.drop(columns=\"unit\")\n",
    "\n",
    "#One-hot for month\n",
    "\n",
    "#Include column for days\n",
    "receival_purchase_order_df[\"days_as_datetime\"] = pd.to_datetime(receival_purchase_order_df[\"date_arrival\"], errors=\"coerce\", utc=True)\n",
    "#Define first and last days\n",
    "first_day = receival_purchase_order_df[\"days_as_datetime\"].min()\n",
    "last_day = receival_purchase_order_df[\"days_as_datetime\"].max()\n",
    "#Normalize days since start\n",
    "receival_purchase_order_df[\"days_normalized\"] = (receival_purchase_order_df[\"days_as_datetime\"]-first_day).dt.days\n",
    "\n",
    "print(\"--------------------\")\n",
    "#receival_purchase_order_df.info()\n",
    "simple_cleaned_df = receival_purchase_order_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2142b",
   "metadata": {},
   "source": [
    "### 1.2 Data-preparation: Option 1- Per-day, Per-receival\n",
    "We can make further preparations to our dataset if we only care about each \"day\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d24a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show what columns are in our dataframe\n",
    "#simple_cleaned_df.info()\n",
    "\n",
    "#We will group multiple orders together as we will only include receivals per day. That means we can drop or change some rows.\n",
    "Option1_cleaning = simple_cleaned_df.drop(columns=(\n",
    "    [\"product_id_x\", #Messy when grouping receivals per day. Should ideally include?\n",
    "    #\"purchase_order_id\", #Messy when grouping\n",
    "    \"purchase_order_item_no\", #Messy when grouping\n",
    "    \"receival_item_no\", #Could include if summarizing makes sense\n",
    "    \"batch_id\", #Very many missing, Could include if summarizing makes sense\n",
    "    \"receival_status\", #Instructed that all are valid\n",
    "    \"supplier_id\", #Messy when grouping\n",
    "    \"product_id_y\", #Should just match first product_id\n",
    "    \"created_date_time\", #Unlikely to be relevant, messy when grouping\n",
    "    \"modified_date_time\", #Unlikely to be relevant, messy when grouping\n",
    "    \"unit_id\", #All should be KG now\n",
    "    \"unit\", #All should be KG now\n",
    "    \"status_id\", #unclear what this does\n",
    "    \"status\", #should be handled in prior code\n",
    "    \"product_version\", #Messy when grouping\n",
    "    \"date_arrival\", #Redundant with days_as_datetime\n",
    "    ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47efa8",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering- Preparing new data_frame for per_day, per_id.\n",
    "We make a new dataframe where for we include each day (roghly 20 years) and each id (203) for a total of 203*~7000 rows. Many of these will be zeros, as there are not deliveries every day, but the \"format\" will match our desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066d303",
   "metadata": {},
   "source": [
    "### 2.1 Aggregates:\n",
    "With the suggested format, we group all receivals per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51bac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_16936\\1440422999.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_aggregates[\"average_orders\"].fillna(0, inplace=True)\n",
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_16936\\1440422999.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_aggregates[\"max_orders\"].fillna(0, inplace=True)\n",
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_16936\\1440422999.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_aggregates[\"std_orders\"].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "daily_aggregates = Option1_cleaning.groupby(['rm_id', 'days_normalized']).agg(\n",
    "    #The total net_weight for that material on that day\n",
    "    daily_weight=('net_weight', 'sum'),\n",
    "    #Number of orders\n",
    "    num_orders=('purchase_order_id', 'count'),\n",
    "\n",
    "    #General metrics about the orders\n",
    "    average_orders=('net_weight', 'mean'),\n",
    "    max_orders=('net_weight', 'max'),\n",
    "    std_orders=('net_weight', 'std')\n",
    ").reset_index()\n",
    "#daily_aggregates = daily_aggregates.fillna({'daily_weight': 0, 'num_orders': 0, 'avg_order_size': 0, 'max_order_size': 0, 'std_order_size': 0}, inplace=True)\n",
    "daily_aggregates[\"average_orders\"].fillna(0, inplace=True)\n",
    "daily_aggregates[\"max_orders\"].fillna(0, inplace=True)\n",
    "daily_aggregates[\"std_orders\"].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46634b",
   "metadata": {},
   "source": [
    "## 2.1 \"Common\" Features\n",
    "Some features are decided by dates, and not dependent on rm_id. We create these first to avoid doing this in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8deff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first day is:  2004-06-15 11:34:00+00:00\n",
      "237 days were marked as holidays\n"
     ]
    }
   ],
   "source": [
    "#Must run pip install holidays\n",
    "import holidays\n",
    "#We get all holidays for a relevant year-range:\n",
    "years = list(range(2004, 2026 + 1))\n",
    "all_holidays = holidays.Norway(years = years)\n",
    "\n",
    "\n",
    "print(\"The first day is: \" , first_day)\n",
    "#We start with finding the range of days\n",
    "full_range = pd.date_range(start=first_day, end=last_day)\n",
    "full_range = pd.to_datetime(full_range)\n",
    "#We want days as \n",
    "full_range_days = (full_range-first_day).days\n",
    "#Remember when we make predictions we should present each day in terms of days since first_day\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Some features are the same for all ID's we create them once to not have to do it many times.\n",
    "#We should have days normalized, and date\n",
    "df_for_all_ids = pd.DataFrame({'days_normalized' : full_range_days})\n",
    "df_for_all_ids[\"date\"] = full_range\n",
    "#We should get what day of the week it is, since this is likely a good predictor\n",
    "df_for_all_ids[\"day_of_week\"] = df_for_all_ids[\"date\"].dt.dayofweek\n",
    "#Weekend is perhaps even stronger predictor: So we create a seperate column for this\n",
    "df_for_all_ids[\"is_weekend\"]  = df_for_all_ids['day_of_week'].isin([5, 6])\n",
    "df_for_all_ids[\"is_weekend\"] = df_for_all_ids[\"is_weekend\"].astype(int)\n",
    "df_for_all_ids[\"month\"] = df_for_all_ids[\"date\"].dt.month\n",
    "\n",
    "#To filter out holidays, we have to compare the dates. isin() is apparently deprecated for datetime so we convert both to strings.\n",
    "df_for_all_ids[\"date_str\"] = df_for_all_ids[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "holiday_str = set(pd.to_datetime(list(all_holidays.keys())).strftime('%Y-%m-%d'))\n",
    "df_for_all_ids[\"is_Holiday\"] = df_for_all_ids[\"date_str\"].isin(holiday_str)\n",
    "df_for_all_ids[\"is_Holiday\"] = df_for_all_ids[\"is_Holiday\"].astype(int)\n",
    "#Verify py printing how many were marked as holidays\n",
    "print(df_for_all_ids[\"is_Holiday\"].sum(), \"days were marked as holidays\")\n",
    "\n",
    "\n",
    "\n",
    "#Time since last purchase or rolling window..\n",
    "#How do compensate for our predictions being outside our training data (more specificly AFTER). \n",
    "#Should cross validation still be usedd or should we prioritze later dates more in a way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84169ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make our dataframe, we make an empty list we can append to.\n",
    "df_full = []\n",
    "\n",
    "\n",
    "#For each ID we create fields for all days.\n",
    "#for i in range(0, len(valid_ids[0:5:1])):\n",
    "for i in valid_ids:\n",
    "    #We create sub-group dataframes that we will append to the list.\n",
    "    #We copy data from the \"felles\" dataframes\n",
    "    df_i = df_for_all_ids\n",
    "\n",
    "    #We include the rm_id\n",
    "    df_i[\"rm_id\"] = i\n",
    "\n",
    "    #df_i[\"rm_id\"] = valid_ids[i]\n",
    "\n",
    "    #For days where we actually have receivals, we get those receivals from the daily_aggregates dataframe\n",
    "    df_i = df_i.merge(daily_aggregates[daily_aggregates['rm_id'] == i], on=['rm_id', 'days_normalized'], how='left')\n",
    "\n",
    "    #If there are no receivals in a day, we add a daily weight of 0. as well as zero for the other aggregate metrics.\n",
    "    df_i[\"daily_weight\"] = df_i[\"daily_weight\"].fillna(0)\n",
    "    df_i[\"average_orders\"] = df_i[\"average_orders\"].fillna(0)\n",
    "    df_i[\"max_orders\"] = df_i[\"max_orders\"].fillna(0)\n",
    "    df_i[\"std_orders\"] = df_i[\"std_orders\"].fillna(0)\n",
    "    df_i[\"num_orders\"] = df_i[\"num_orders\"].fillna(0)\n",
    "\n",
    "\n",
    "    #We create a column for cumulative weight, although I'm not sure it should be included\n",
    "    df_i['cumulative_weight'] = df_i['daily_weight'].cumsum()\n",
    "\n",
    "    #Rolling windows. We create 3 rolling windows, one week, one month, and one 100 days.\n",
    "    df_i[\"rolling_7\"] = df_i['daily_weight'].rolling(window = 7).mean()\n",
    "    df_i[\"rolling_30\"] = df_i['daily_weight'].rolling(window = 30).mean()\n",
    "    df_i[\"rolling_100\"] = df_i['daily_weight'].rolling(window = 100).mean()\n",
    "\n",
    "    #We append to the list and proceed to the next id in the loop\n",
    "    df_full.append(df_i)\n",
    "\n",
    "#When all entries (per_id_per_loop) is added, we create a dataframe of the list.\n",
    "df_final = pd.concat(df_full, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9bb1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1521079 entries, 0 to 1521078\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count    Dtype              \n",
      "---  ------             --------------    -----              \n",
      " 0   days_normalized    1521079 non-null  int64              \n",
      " 1   date               1521079 non-null  datetime64[ns, UTC]\n",
      " 2   day_of_week        1521079 non-null  int32              \n",
      " 3   is_weekend         1521079 non-null  int64              \n",
      " 4   month              1521079 non-null  int32              \n",
      " 5   date_str           1521079 non-null  object             \n",
      " 6   is_Holiday         1521079 non-null  int64              \n",
      " 7   rm_id              1521079 non-null  int64              \n",
      " 8   daily_weight       1521079 non-null  float64            \n",
      " 9   num_orders         44188 non-null    float64            \n",
      " 10  average_orders     1521079 non-null  float64            \n",
      " 11  max_orders         1521079 non-null  float64            \n",
      " 12  std_orders         1521079 non-null  float64            \n",
      " 13  cumulative_weight  1521079 non-null  float64            \n",
      " 14  rolling_7          1519861 non-null  float64            \n",
      " 15  rolling_30         1515192 non-null  float64            \n",
      " 16  rolling_100        1500982 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(9), int32(2), int64(4), object(1)\n",
      "memory usage: 185.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#Explore data\n",
    "df_final.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905bc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to file\n",
    "df_final.to_csv(\"cleaned_data/cleaned_df_final_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
