{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6372bea6",
   "metadata": {},
   "source": [
    "# My attempt at creating a \"only time-based\" dataset for predicitons.\n",
    "Perhaps we can mix and match some of the features from this to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fa5288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "\n",
    "#Initial Data\n",
    "receivals_raw_df = pd.read_csv(\"./data/kernel/receivals.csv\")\n",
    "purchase_orders_raw_df = pd.read_csv(\"./data/kernel/purchase_orders.csv\")\n",
    "materials_raw_df = pd.read_csv(\"./data/extended/materials.csv\")\n",
    "transportation_raw_df = pd.read_csv(\"./data/extended/transportation.csv\")\n",
    "\n",
    "prediction_map_raw_df = pd.read_csv(\"./data/prediction_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d2334",
   "metadata": {},
   "source": [
    "# Part 1: Preparing Data.\n",
    "We will try different structures for our dataframe. \n",
    "\n",
    "### Option 1\n",
    "In line with our predictions being \"per-day\" basis, we will create a dataframe where the net-weight per rm_id per day is aggregated, and we will also include entries for every day (even if there are 0 receivals that day). This data structure might help our models since it is much more similar to the output we want to predict, However, it is hard to incorporate info such as purchase order info.\n",
    "\n",
    "### Option 2\n",
    "Dataframe on a \"per-receival\" basis. This is less in-line with our output, but let's us keep more information. We can also use our models to make predictions of receivals and later convert to a per-day basis as a workaround."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661b752",
   "metadata": {},
   "source": [
    "## 1.1 Data Cleaning.\n",
    "Some data-cleanup is required before going with either option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "036c3367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique IDs:  203\n",
      "-57  entries with invalid rm ids dropped\n",
      "-3  duplicates dropped\n",
      "-13  entries with invalid weights dropped\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "valid_ids = prediction_map_raw_df[\"rm_id\"].unique()\n",
    "print(\"Unique IDs: \", len(valid_ids))\n",
    "\n",
    "valid_receivals = receivals_raw_df[receivals_raw_df[\"rm_id\"].isin(valid_ids)]\n",
    "print(len(valid_receivals[\"rm_id\"])- len(receivals_raw_df[\"rm_id\"]), \" entries with invalid rm ids dropped\")\n",
    "\n",
    "#Remove duplicates.\n",
    "drop_duplicates = valid_receivals.drop_duplicates()\n",
    "print(len(drop_duplicates[\"rm_id\"])- len(valid_receivals[\"rm_id\"]), \" duplicates dropped\")\n",
    "\n",
    "#Remove entries where net weight is undefined.\n",
    "valid_weights = drop_duplicates.dropna(subset=[\"net_weight\"])\n",
    "print(len(valid_weights[\"rm_id\"])- len(drop_duplicates[\"rm_id\"]), \" entries with invalid weights dropped\")\n",
    "\n",
    "#Merge with purchase orders\n",
    "receival_purchase_order_df = valid_weights.merge(purchase_orders_raw_df, on=['purchase_order_id', 'purchase_order_item_no'], how=\"left\")\n",
    "\n",
    "#Correct for different weight units\n",
    "receival_purchase_order_df.loc[receival_purchase_order_df['unit'] == 'PUND', 'net_weight'] *= 0.453592\n",
    "receival_purchase_order_df.loc[receival_purchase_order_df['unit'].isna(), 'unit'] = 'kg'\n",
    "receival_purchase_order_df.drop(columns=\"unit\")\n",
    "\n",
    "#One-hot for month\n",
    "\n",
    "#Include column for days\n",
    "receival_purchase_order_df[\"days_as_datetime\"] = pd.to_datetime(receival_purchase_order_df[\"date_arrival\"], errors=\"coerce\", utc=True)\n",
    "#Define first and last days\n",
    "first_day = receival_purchase_order_df[\"days_as_datetime\"].min()\n",
    "last_day = receival_purchase_order_df[\"days_as_datetime\"].max()\n",
    "#Normalize days since start\n",
    "receival_purchase_order_df[\"days_normalized\"] = (receival_purchase_order_df[\"days_as_datetime\"]-first_day).dt.days\n",
    "\n",
    "print(\"--------------------\")\n",
    "#receival_purchase_order_df.info()\n",
    "simple_cleaned_df = receival_purchase_order_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2142b",
   "metadata": {},
   "source": [
    "### 1.2 Data-preparation: Option 1- Per-day, Per-receival\n",
    "We can make further preparations to our dataset if we only care about each \"day\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6d24a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show what columns are in our dataframe\n",
    "#simple_cleaned_df.info()\n",
    "\n",
    "#We will group multiple orders together as we will only include receivals per day. That means we can drop or change some rows.\n",
    "Option1_cleaning = simple_cleaned_df.drop(columns=(\n",
    "    [\"product_id_x\", #Messy when grouping receivals per day. Should ideally include?\n",
    "    #\"purchase_order_id\", #Messy when grouping\n",
    "    \"purchase_order_item_no\", #Messy when grouping\n",
    "    \"receival_item_no\", #Could include if summarizing makes sense\n",
    "    \"batch_id\", #Very many missing, Could include if summarizing makes sense\n",
    "    \"receival_status\", #Instructed that all are valid\n",
    "    \"supplier_id\", #Messy when grouping\n",
    "    \"product_id_y\", #Should just match first product_id\n",
    "    \"created_date_time\", #Unlikely to be relevant, messy when grouping\n",
    "    \"modified_date_time\", #Unlikely to be relevant, messy when grouping\n",
    "    \"unit_id\", #All should be KG now\n",
    "    \"unit\", #All should be KG now\n",
    "    \"status_id\", #unclear what this does\n",
    "    \"status\", #should be handled in prior code\n",
    "    \"product_version\", #Messy when grouping\n",
    "    \"date_arrival\", #Redundant with days_as_datetime\n",
    "    ]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47efa8",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering- Preparing new data_frame for per_day, per_id.\n",
    "We make a new dataframe where for we include each day (roghly 20 years) and each id (203) for a total of 203*~7000 rows. Many of these will be zeros, as there are not deliveries every day, but the \"format\" will match our desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066d303",
   "metadata": {},
   "source": [
    "### 2.1 Aggregates:\n",
    "With the suggested format, we group all receivals per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44188 entries, 0 to 44187\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   rm_id            44188 non-null  float64\n",
      " 1   days_normalized  44188 non-null  int64  \n",
      " 2   daily_weight     44188 non-null  float64\n",
      " 3   num_orders       44188 non-null  int64  \n",
      " 4   average_orders   44188 non-null  float64\n",
      " 5   max_orders       44188 non-null  float64\n",
      " 6   std_orders       44188 non-null  float64\n",
      "dtypes: float64(5), int64(2)\n",
      "memory usage: 2.4 MB\n",
      "None\n",
      "0        0.0\n",
      "1        0.0\n",
      "2        0.0\n",
      "3        0.0\n",
      "4        0.0\n",
      "        ... \n",
      "44183    0.0\n",
      "44184    0.0\n",
      "44185    0.0\n",
      "44186    0.0\n",
      "44187    0.0\n",
      "Name: std_orders, Length: 44188, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_8808\\3324819586.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_aggregates[\"average_orders\"].fillna(0, inplace=True)\n",
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_8808\\3324819586.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_aggregates[\"max_orders\"].fillna(0, inplace=True)\n",
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_8808\\3324819586.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  daily_aggregates[\"std_orders\"].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "daily_aggregates = Option1_cleaning.groupby(['rm_id', 'days_normalized']).agg(\n",
    "    #The total net_weight for that material on that day\n",
    "    daily_weight=('net_weight', 'sum'),\n",
    "    #Number of orders\n",
    "    num_orders=('purchase_order_id', 'count'),\n",
    "\n",
    "    #General metrics about the orders\n",
    "    average_orders=('net_weight', 'mean'),\n",
    "    max_orders=('net_weight', 'max'),\n",
    "    std_orders=('net_weight', 'std')\n",
    ").reset_index()\n",
    "#daily_aggregates = daily_aggregates.fillna({'daily_weight': 0, 'num_orders': 0, 'avg_order_size': 0, 'max_order_size': 0, 'std_order_size': 0}, inplace=True)\n",
    "daily_aggregates[\"average_orders\"].fillna(0, inplace=True)\n",
    "daily_aggregates[\"max_orders\"].fillna(0, inplace=True)\n",
    "daily_aggregates[\"std_orders\"].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46634b",
   "metadata": {},
   "source": [
    "## 2.1 \"Common\" Features\n",
    "Some features are decided by dates, and not dependent on rm_id. We create these first to avoid doing this in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8deff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first day is:  2004-06-15 11:34:00+00:00\n",
      "237 days were marked as holidays\n"
     ]
    }
   ],
   "source": [
    "#Must run pip install holidays\n",
    "import holidays\n",
    "#We get all holidays for a relevant year-range:\n",
    "years = list(range(2004, 2026 + 1))\n",
    "all_holidays = holidays.Norway(years = years)\n",
    "\n",
    "\n",
    "print(\"The first day is: \" , first_day)\n",
    "#We start with finding the range of days\n",
    "full_range = pd.date_range(start=first_day, end=last_day)\n",
    "full_range = pd.to_datetime(full_range)\n",
    "#We want days as \n",
    "full_range_days = (full_range-first_day).days\n",
    "#Remember when we make predictions we should present each day in terms of days since first_day\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Some features are the same for all ID's we create them once to not have to do it many times.\n",
    "#We should have days normalized, and date\n",
    "df_for_all_ids = pd.DataFrame({'days_normalized' : full_range_days})\n",
    "df_for_all_ids[\"date\"] = full_range\n",
    "#We should get what day of the week it is, since this is likely a good predictor\n",
    "df_for_all_ids[\"day_of_week\"] = df_for_all_ids[\"date\"].dt.dayofweek\n",
    "#Weekend is perhaps even stronger predictor: So we create a seperate column for this\n",
    "df_for_all_ids[\"is_weekend\"]  = df_for_all_ids['day_of_week'].isin([5, 6])\n",
    "df_for_all_ids[\"is_weekend\"] = df_for_all_ids[\"is_weekend\"].astype(int)\n",
    "df_for_all_ids[\"month\"] = df_for_all_ids[\"date\"].dt.month\n",
    "\n",
    "#To filter out holidays, we have to compare the dates. isin() is apparently deprecated for datetime so we convert both to strings.\n",
    "df_for_all_ids[\"date_str\"] = df_for_all_ids[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "holiday_str = set(pd.to_datetime(list(all_holidays.keys())).strftime('%Y-%m-%d'))\n",
    "df_for_all_ids[\"is_Holiday\"] = df_for_all_ids[\"date_str\"].isin(holiday_str)\n",
    "df_for_all_ids[\"is_Holiday\"] = df_for_all_ids[\"is_Holiday\"].astype(int)\n",
    "#Verify py printing how many were marked as holidays\n",
    "print(df_for_all_ids[\"is_Holiday\"].sum(), \"days were marked as holidays\")\n",
    "\n",
    "\n",
    "\n",
    "#Time since last purchase or rolling window..\n",
    "#How do compensate for our predictions being outside our training data (more specificly AFTER). \n",
    "#Should cross validation still be usedd or should we prioritze later dates more in a way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "84169ec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 23.2 MiB for an array with shape (2, 1521079) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     df_full.append(df_i)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m#When all entries (per_id_per_loop) is added, we create a dataframe of the list.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m df_final = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aksel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = False\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m return op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aksel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aksel\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:177\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    167\u001b[39m vals = [ju.block.values \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk.is_extension:\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     values = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk.dtype):\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[32m    180\u001b[39m     values = concat_compat(vals, axis=\u001b[32m0\u001b[39m, ea_compat_axis=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 23.2 MiB for an array with shape (2, 1521079) and data type int64"
     ]
    }
   ],
   "source": [
    "#To make our dataframe, we make an empty list we can append to.\n",
    "df_full = []\n",
    "\n",
    "\n",
    "#For each ID we create fields for all days.\n",
    "#for i in range(0, len(valid_ids[0:5:1])):\n",
    "for i in valid_ids:\n",
    "    #We create sub-group dataframes that we will append to the list.\n",
    "    #We copy data from the \"felles\" dataframes\n",
    "    df_i = df_for_all_ids\n",
    "\n",
    "    #We include the rm_id\n",
    "    df_i[\"rm_id\"] = i\n",
    "\n",
    "    #df_i[\"rm_id\"] = valid_ids[i]\n",
    "\n",
    "    #For days where we actually have receivals, we get those receivals from the daily_aggregates dataframe\n",
    "    df_i = df_i.merge(daily_aggregates[daily_aggregates['rm_id'] == i], on=['rm_id', 'days_normalized'], how='left')\n",
    "\n",
    "    #If there are no receivals in a day, we add a daily weight of 0. as well as zero for the other aggregate metrics.\n",
    "    df_i[\"daily_weight\"] = df_i[\"daily_weight\"].fillna(0)\n",
    "    df_i[\"average_orders\"] = df_i[\"average_orders\"].fillna(0)\n",
    "    df_i[\"max_orders\"] = df_i[\"max_orders\"].fillna(0)\n",
    "    df_i[\"std_orders\"] = df_i[\"std_orders\"].fillna(0)\n",
    "\n",
    "\n",
    "    #We create a column for cumulative weight, although I'm not sure it should be included\n",
    "    df_i['cumulative_weight'] = df_i['daily_weight'].cumsum()\n",
    "\n",
    "    #Rolling windows. We create 3 rolling windows, one week, one month, and one 100 days.\n",
    "    df_i[\"rolling_7\"] = df_i['daily_weight'].rolling(window = 7).mean()\n",
    "    df_i[\"rolling_30\"] = df_i['daily_weight'].rolling(window = 30).mean()\n",
    "    df_i[\"rolling_100\"] = df_i['daily_weight'].rolling(window = 100).mean()\n",
    "\n",
    "    #We append to the list and proceed to the next id in the loop\n",
    "    df_full.append(df_i)\n",
    "\n",
    "#When all entries (per_id_per_loop) is added, we create a dataframe of the list.\n",
    "df_final = pd.concat(df_full, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9bb1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37465 entries, 0 to 37464\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype              \n",
      "---  ------             --------------  -----              \n",
      " 0   days_normalized    37465 non-null  int64              \n",
      " 1   date               37465 non-null  datetime64[ns, UTC]\n",
      " 2   day_of_week        37465 non-null  int32              \n",
      " 3   is_weekend         37465 non-null  int64              \n",
      " 4   month              37465 non-null  int32              \n",
      " 5   date_str           37465 non-null  object             \n",
      " 6   is_Holiday         37465 non-null  int64              \n",
      " 7   rm_id              37465 non-null  int64              \n",
      " 8   daily_weight       37465 non-null  float64            \n",
      " 9   num_orders         0 non-null      float64            \n",
      " 10  average_orders     37465 non-null  float64            \n",
      " 11  max_orders         37465 non-null  float64            \n",
      " 12  std_orders         37465 non-null  float64            \n",
      " 13  cumulative_weight  37465 non-null  float64            \n",
      " 14  rolling_7          37435 non-null  float64            \n",
      " 15  rolling_30         37320 non-null  float64            \n",
      " 16  rolling_100        36970 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(9), int32(2), int64(4), object(1)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#Explore data\n",
    "df_final.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "905bc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print to file\n",
    "df_final.to_csv(\"cleaned_data/cleaned_df_final_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
